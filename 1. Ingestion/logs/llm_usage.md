# LLM Usage Log

This file records all interactions with the Large Language Model (LLM) for  
**Project: DA-IICT Faculty Scraping**.

Each entry includes the **prompt**, the **complete response (code)**, and **notes**, preserved exactly to ensure transparency, reproducibility, and academic compliance.

---

## Entry 1

- **Date:** 2026-01-16  
- **Tool Used:** ChatGPT (LLM)  
- **Purpose:** Generate Python code to scrape faculty profiles from the DA-IICT website.  
- **Context:** Deterministic web scraping using `requests` and `BeautifulSoup` with strict selector-based extraction, URL-driven faculty categorization, global deduplication, and rule-based parsing.

---

## Prompt + Response

```text
PROMPT:

You are a Data Engineer.
Write COMPLETE, RUNNABLE Python code using requests and BeautifulSoup to scrape all faculty profiles from the DA-IICT website.
The output must be clean JSON suitable for ingestion.
Do not use browser automation.
Do not use shell commands.
The code must run as-is.

1. Faculty category URLs
Scrape all the following listing pages.
Faculty type must be assigned strictly from the listing URL, never inferred from profile pages.
https://www.daiict.ac.in/faculty
faculty_type = core
https://www.daiict.ac.in/adjunct-faculty
faculty_type = adjunct
https://www.daiict.ac.in/adjunct-faculty-international
faculty_type = international
https://www.daiict.ac.in/distinguished-professor
faculty_type = distinguished
https://www.daiict.ac.in/professor-practice
faculty_type = practice

2.Global deduplication
Each faculty member must be scraped exactly once.
Use profile_url as the unique key.
Maintain a global set of visited profile URLs.
If a profile URL is already processed, skip it.

3.Scraping listing pages
On each listing page, locate faculty cards at:
div.facultyInformation > ul > li

4.Scraping profile pages (authoritative source)
Extract fields using explicit selectors.
Do not infer data.
Do not mix sections.

5.Output
Write all data into:
faculty_profiles.json

6.Engineering constraints
Use only requests and BeautifulSoup.
Handle missing fields gracefully.
Catch and log all exceptions.

7.Modularity requirement
Define:
fetch_html
parse_listing_page
parse_profile_page
validate_record
write_output

Generate the complete Python scraping code.

------------------------------------------
RESPONSE (Generated by ChatGPT):
------------------------------------------

import requests
import json
import logging
from bs4 import BeautifulSoup, NavigableString
from datetime import datetime
from typing import List, Dict, Set, Optional

OUTPUT_FILE = "faculty_profiles.json"
TIMEOUT = 20

LISTING_PAGES = [
    ("https://www.daiict.ac.in/faculty", "core"),
    ("https://www.daiict.ac.in/adjunct-faculty", "adjunct"),
    ("https://www.daiict.ac.in/adjunct-faculty-international", "international"),
    ("https://www.daiict.ac.in/distinguished-professor", "distinguished"),
    ("https://www.daiict.ac.in/professor-practice", "practice"),
]

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s"
)

def fetch_html(url: str) -> Optional[str]:
    try:
        resp = requests.get(url, timeout=TIMEOUT)
        if resp.status_code != 200:
            logging.error(f"Non-200 response [{resp.status_code}] for URL: {url}")
            return None
        return resp.text
    except Exception:
        logging.exception(f"Failed to fetch URL: {url}")
        return None

def parse_listing_page(html: str, listing_url: str, faculty_type: str) -> List[Dict]:
    soup = BeautifulSoup(html, "html.parser")
    records = []
    for li in soup.select("div.facultyInformation > ul > li"):
        a = li.select_one("h3 a")
        if not a or not a.get("href"):
            continue
        records.append({
            "name": a.get_text(strip=True),
            "profile_url": a["href"].strip(),
            "faculty_type": faculty_type,
            "source_listing_url": listing_url
        })
    return records

def parse_profile_page(html: str, profile_url: str, faculty_type: str, source_listing_url: str) -> Optional[Dict]:
    soup = BeautifulSoup(html, "html.parser")

    def safe(sel):
        el = soup.select_one(sel)
        return el.get_text(strip=True) if el else ""

    biography_el = soup.select_one(".field--name-field-biography")
    biography = biography_el.get_text(strip=True) if biography_el else None

    specialization = ""
    for h2 in soup.find_all("h2"):
        if h2.get_text(strip=True) == "Specialization":
            parent = h2.find_parent("div", class_="specializationIcon")
            if parent:
                nxt = parent.find_next_sibling("div", class_="work-exp")
                if nxt:
                    specialization = " ".join(nxt.stripped_strings)
            break

    publications = []
    for h2 in soup.find_all("h2"):
        if h2.get_text(strip=True) == "Publications":
            ul = h2.find_next("ul", class_="bulletText")
            if ul:
                publications = [li.get_text(strip=True) for li in ul.find_all("li", recursive=False)]
            break

    teaching = []
    teaching_el = soup.select_one(".field--name-field-teaching")
    if teaching_el:
        for node in teaching_el.descendants:
            if isinstance(node, NavigableString):
                txt = node.strip()
                if txt:
                    teaching.append(txt)

    return {
        "name": safe(".field--name-field-faculty-names"),
        "faculty_type": faculty_type,
        "education": safe(".field--name-field-faculty-name"),
        "phone": safe(".field--name-field-contact-no"),
        "email": safe(".field--name-field-email .field__item"),
        "address": safe(".field--name-field-address"),
        "specialization": specialization,
        "profile_url": profile_url,
        "biography": biography,
        "publications": publications,
        "teaching": teaching,
        "source_listing_url": source_listing_url,
        "scraped_at": datetime.utcnow().isoformat()
    }

def validate_record(r: Dict) -> bool:
    keys = [
        "name","faculty_type","education","phone","email","address",
        "specialization","profile_url","biography","publications",
        "teaching","source_listing_url","scraped_at"
    ]
    return all(k in r for k in keys)

def write_output(records: List[Dict], filename: str):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(records, f, indent=2, ensure_ascii=False)

def main():
    visited: Set[str] = set()
    final: List[Dict] = []

    for url, ftype in LISTING_PAGES:
        html = fetch_html(url)
        if not html:
            return
        for entry in parse_listing_page(html, url, ftype):
            if entry["profile_url"] in visited:
                continue
            visited.add(entry["profile_url"])
            prof_html = fetch_html(entry["profile_url"])
            if not prof_html:
                continue
            rec = parse_profile_page(
                prof_html,
                entry["profile_url"],
                entry["faculty_type"],
                entry["source_listing_url"]
            )
            if rec and validate_record(rec):
                final.append(rec)

    write_output(final, OUTPUT_FILE)

if __name__ == "__main__":
    main()


