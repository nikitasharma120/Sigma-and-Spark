# LLM Usage Log

This file records all interactions with the Large Language Model (LLM) for  
**Project: DA-IICT Faculty Data Cleaning & Transformation**.

Each entry documents **human analysis notes**, the **prompt**, and the **complete LLM response (code)** to ensure transparency, reproducibility, and academic compliance.

---

## Entry 1

- **Date:** 2026-01-16  
- **Tool Used:** ChatGPT (LLM)  
- **Purpose:** Generate Python code to clean and transform scraped faculty profile data stored in JSON format.  
- **Context:** Data cleaning and normalization — schema enforcement, string normalization, list cleaning, and safe restructuring without inference.

---


## Prompt + Response

```text
PROMPT:

You are a Data Transformation Assistant.
Generate Python code to clean faculty profile data stored in JSON.

File handling:
Input file: Data/faculty_profiles.json
Output file: Data/faculty_cleaned.json
Use OS-safe, project-relative paths (the script may live inside a subfolder).

Cleaning rules (apply to every record):
Normalize: name, education, biography, specialization, teaching, publications, contact, profile_url
Replace missing, empty, null, or placeholder values with:
"Not Available" for string fields
[] for list fields
Remove HTML tags, weird characters, and extra whitespace
Separate education and biography if they are mixed
Ensure teaching and publications are lists of clean strings
Merge phone, email, and address into a single contact object
Remove only # symbols from addresses (do NOT remove room numbers or other address information)
Do not infer or hallucinate missing data

Output schema (strict):
{
  "name": "",
  "education": "",
  "biography": "",
  "specialization": "",
  "teaching": [],
  "publications": [],
  "contact": { "phone": "", "email": "", "address": "" },
  "profile_url": ""
}

Generate the complete Python data cleaning and transformation code.

------------------------------------------
RESPONSE (Generated by ChatGPT):
------------------------------------------

import json
import os
import re
from typing import Any, Dict, List

# -------------------------------------------------------------------
# Path handling (OS-safe, project-relative)
# -------------------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INPUT_PATH = os.path.join(BASE_DIR, "..", "Data", "faculty_profiles.json")
OUTPUT_PATH = os.path.join(BASE_DIR, "..", "Data", "faculty_cleaned.json")

# -------------------------------------------------------------------
# Constants
# -------------------------------------------------------------------
NA_STRING = "Not Available"

# -------------------------------------------------------------------
# Utility cleaning functions
# -------------------------------------------------------------------
def normalize_whitespace(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()


def strip_html(text: str) -> str:
    return re.sub(r"<[^>]+>", "", text)


def clean_string(value: Any) -> str:
    if not value or not isinstance(value, str):
        return NA_STRING

    cleaned = strip_html(value)
    cleaned = normalize_whitespace(cleaned)

    if cleaned in {"", "null", "None", "-", "--"}:
        return NA_STRING

    return cleaned


def clean_address(address: Any) -> str:
    if not address or not isinstance(address, str):
        return NA_STRING

    # Remove ONLY '#' symbols
    address = address.replace("#", "")
    address = strip_html(address)
    address = normalize_whitespace(address)

    return address if address else NA_STRING


def clean_list(values: Any) -> List[str]:
    if not isinstance(values, list):
        return []

    cleaned_items = []
    for item in values:
        if isinstance(item, str):
            text = normalize_whitespace(strip_html(item))
            if text:
                cleaned_items.append(text)

    return cleaned_items


def separate_education_and_biography(education: str, biography: str) -> (str, str):
    """
    If education text is polluted with biography-like sentences,
    split conservatively without inferring new content.
    """
    if education == NA_STRING:
        return NA_STRING, biography

    if biography != NA_STRING:
        return education, biography

    if education.count(".") >= 2:
        parts = education.split(".", 1)
        edu = normalize_whitespace(parts[0])
        bio = normalize_whitespace(parts[1])

        return edu or NA_STRING, bio or NA_STRING

    return education, biography


# -------------------------------------------------------------------
# Record transformation
# -------------------------------------------------------------------
def transform_record(record: Dict[str, Any]) -> Dict[str, Any]:
    name = clean_string(record.get("name"))
    education_raw = clean_string(record.get("education"))
    biography_raw = clean_string(record.get("biography"))
    specialization = clean_string(record.get("specialization"))
    profile_url = clean_string(record.get("profile_url"))

    education, biography = separate_education_and_biography(
        education_raw,
        biography_raw
    )

    teaching = clean_list(record.get("teaching"))
    publications = clean_list(record.get("publications"))

    contact = {
        "phone": clean_string(record.get("phone")),
        "email": clean_string(record.get("email")),
        "address": clean_address(record.get("address"))
    }

    return {
        "name": name,
        "education": education,
        "biography": biography,
        "specialization": specialization,
        "teaching": teaching,
        "publications": publications,
        "contact": contact,
        "profile_url": profile_url
    }


# -------------------------------------------------------------------
# Main execution
# -------------------------------------------------------------------
def main():
    if not os.path.exists(INPUT_PATH):
        raise FileNotFoundError(f"Input file not found: {INPUT_PATH}")

    with open(INPUT_PATH, "r", encoding="utf-8") as f:
        raw_records = json.load(f)

    cleaned_records = []
    for record in raw_records:
        try:
            cleaned_records.append(transform_record(record))
        except Exception:
            continue

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(cleaned_records, f, indent=2, ensure_ascii=False)

    print(f"Cleaned {len(cleaned_records)} records → {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
